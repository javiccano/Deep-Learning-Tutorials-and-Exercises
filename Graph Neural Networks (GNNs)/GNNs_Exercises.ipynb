{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNMD5M6mQ2YY"
   },
   "source": [
    "# Grap Neural Networks (GNNs) Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKehhGDF-Qte"
   },
   "source": [
    "These exercises are separated into a coding and a theory component.\n",
    "\n",
    "### Part 1 - Coding\n",
    "In this part you will have to:\n",
    "\n",
    "#### Part 1.1\n",
    "\n",
    "Implement the Graph Convolutional Networks [Kipf & Welling](http://arxiv.org/abs/1609.02907) (ICLR 2017).\n",
    "\n",
    "#### Part 1.2\n",
    "\n",
    "Implement the Graph Attentional Networks ([Veličković et al.](https://arxiv.org/abs/1710.10903) (ICLR 2018)).\n",
    "\n",
    "\n",
    "### Part 2 - Theory \n",
    "\n",
    "Here you will answer some theoretical questions about graph attentional networks -- no detailed proofs and no programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qaI4P8SZ-U2j"
   },
   "source": [
    "# Part 1: Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coursework is partially based on the repo created by Bumsoo Kim, Ph.D Candidate in Korea University: https://github.com/tkipf/pygcn](https://github.com/tkipf/pygcn).\n",
    "\n",
    "## Graph Convolutional Networks\n",
    "\n",
    "As we explained in the tutorial, most graph neural network models have a somewhat universal architecture in common. They are referred as Graph Convoutional Networks (GCNs) since filter parameters are typically shared over all locations in the graph.\n",
    "\n",
    "<p align=\"center\"><img width=\"80%\" src=\"gcn_web.png\"></p>\n",
    "\n",
    "For these models, the goal is to learn a function of signals/features on a graph $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E})$ with $N$ nodes $v_i \\in \\mathcal{V}$, edges $(v_i,v_j)\\in \\mathcal{V}$. This graph takes as:\n",
    "\n",
    "**Input**\n",
    "- $N \\times D$ feature matrix ($N$ : Number of nodes, $D$ : number of features per node)\n",
    "- Representative description of the graph structure in matrix form; typically in the form of $N \\times N$ adjacency matrix $A$\n",
    "\n",
    "**Output**\n",
    "- $N \\times F$ feature matrix ($N$ : Number of nodes, $F$ : number of output features)\n",
    "\n",
    "Graph-level outputs can be modeled by introducing some form of pooling operation.\n",
    "\n",
    "\n",
    "For more details, see [here](https://tkipf.github.io/graph-convolutional-networks/).\n",
    "\n",
    "\n",
    "\n",
    "## Transductive learning on citation networks\n",
    "In this coursework, we use an implementation of Planetoid, a graph-based semi-supervised learning framework proposed in the following paper: [Revisiting Semi-Supervised Learning with Graph Embeddings](https://arxiv.org/abs/1603.08861).\n",
    "\n",
    "This dataset is consisted of 3 sub-datasets ('pubmed', 'cora', 'citeseer'). Specifically, we will be working with the *Cora* citation network ([Sen et al.](http://eliassi.org/papers/ai-mag-tr08.pdf)) for the task of transductive node classification, which contains $2,708$ nodes, $5,429$ edges, $7$ classes and $1,433$ features per node. We will use $140$ instances for training, $500$ for validation, and $1,000$ for testing.\n",
    "\n",
    "Each node in the dataset represents a document, and the edge represents the 'reference' relationship between the documents.\n",
    "\n",
    "For more details, see [here](https://github.com/kimiyoung/planetoid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up code and imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "use_gpu = torch.cuda.is_available() # if you have available GPUs...\n",
    "\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "## Part 1.1: GCNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### Pre-processing\n",
    "\n",
    "First of all, download the planetoid dataset in the \"data\" folder from https://github.com/kimiyoung/planetoid and assign to *dataroot* the directory to the downloaded dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = '' # absolute path where we have stored the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the propagation rule introduced in [Kipf & Welling](http://arxiv.org/abs/1609.02907) (ICLR 2017):\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma\\left( \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}\\right),\n",
    "$$\n",
    "\n",
    "where $\\sigma(\\cdot)$ denotes the activation function. Therefore, we need first to compute the filter $\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$, wich will be constant across the different layers of the GCN:\n",
    "\n",
    "- $\\tilde{A}=A+I_N$ is the adjacency matrix of the undirected graph $\\mathcal{G}$ with added self-connections. $I_N$ is the identity matrix\n",
    "- $\\tilde{D}_{ii}=\\sum_j \\tilde{A}_{ij}$ is the degree matrix of $\\mathcal{G}$ with added self-connections\n",
    "\n",
    "Now it is your turn to compute the aforementioned filter from the adjacency matrix $A$. To improve numerical stability, after computing $\\tilde{D}^{-\\frac{1}{2}}$ we recommend:\n",
    "1. Check if $\\tilde{D}^{-\\frac{1}{2}}$ has infinity values and substitute them by zero\n",
    "2. From the resulting matrix, create a sparse matrix based on its diagonal elements.\n",
    "3. Compute $\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$\n",
    "4. Or you can use your imagination to solve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    ########################################################################\n",
    "    ## START OF YOUR CODE\n",
    "    ########################################################################\n",
    "    \n",
    "    \n",
    "    ########################################################################\n",
    "    ## END OF YOUR CODE\n",
    "    ########################################################################\n",
    "\n",
    "    return adj.tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to (row-)normalize the input feature vectors, as in [Kipf & Welling](http://arxiv.org/abs/1609.02907) (ICLR 2017):\n",
    "\n",
    "\n",
    "$$\n",
    "\\tilde{X}=  D_X^{-1}X,\n",
    "$$\n",
    "where $D_{X,ii}=\\sum_jX_{ij}$\n",
    "\n",
    "Again, you should:\n",
    "1. Check that $D_X^{-1}$ has infinity values and substitute them by zero\n",
    "2. From the resulting matrix, create a sparse matrix based on its diagonal elements.\n",
    "3. Compute $D_X^{-1}X$\n",
    "4. Or you can use your imagination to solve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feat(features):\n",
    "    \n",
    "    ########################################################################\n",
    "    ## START OF YOUR CODE\n",
    "    ########################################################################\n",
    "    \n",
    "    \n",
    "    ########################################################################\n",
    "    ## END OF YOUR CODE\n",
    "    ########################################################################\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this normalise version of features as the input for the graph. Next, we can pre-process the graph data. You are expected to normalise the adjacency matrix and the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=dataroot, dataset=\"cora\"):\n",
    "    \"\"\"\n",
    "    x     => the feature vectors of the training instances (scipy.sparse.csr.csr_matrix)\n",
    "    y     => the one-hot labels of the labeled training instances (numpy.ndarray)\n",
    "    tx => the feature vectors of the test instances (scipy.sparse.csr.csr_matrix)\n",
    "    ty => the one-hot labels of the test instances (numpy.ndarray)\n",
    "    allx  => the feature vectors of both labeled and unlabeled training instances (csr_matrix)\n",
    "    ally  => the labels for instances in ind.dataset_str.allx (numpy.ndarray)\n",
    "    graph => the dict in the format {index: [index of neighbor nodes]} (collections.defaultdict)\n",
    "    \"\"\"\n",
    "    print(\"\\n[STEP 1]: Upload {} dataset.\".format(dataset))\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        print(\"{}/ind.{}.{}\".format(path, dataset, names[i]))\n",
    "        with open(\"{}/ind.{}.{}\".format(path, dataset, names[i]), 'rb') as f:\n",
    "            \n",
    "            u = pkl._Unpickler(f)\n",
    "            u.encoding = 'latin1'\n",
    "            p = u.load()\n",
    "            objects.append(p)\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    \n",
    "    test_idx_reorder = []\n",
    "\n",
    "    for line in open(\"{}/ind.{}.test.index\".format(path, dataset)):\n",
    "        test_idx_reorder.append(int(line.strip()))\n",
    "\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    \n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))    \n",
    "    \n",
    "    ########################################################################\n",
    "    ## START OF YOUR CODE\n",
    "    ########################################################################\n",
    "\n",
    "    \n",
    "    ########################################################################\n",
    "    ## END OF YOUR CODE\n",
    "    ########################################################################\n",
    "    \n",
    "    print(\"| # of nodes : {}\".format(adj.shape[0]))\n",
    "    print(\"| # of edges : {}\".format(adj.sum().sum()/2))\n",
    "    print(\"| # of features : {}\".format(features.shape[1]))\n",
    "    print(\"| # of clases   : {}\".format(ally.shape[1]))\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    sparse_mx = adj.tocoo().astype(np.float32)\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "    idx_test = test_idx_range.tolist()\n",
    "\n",
    "    print(\"| # of train set : {}\".format(len(idx_train)))\n",
    "    print(\"| # of val set   : {}\".format(len(idx_val)))\n",
    "    print(\"| # of test set  : {}\".format(len(idx_test)))\n",
    "\n",
    "    idx_train, idx_val, idx_test = list(map(lambda x: torch.LongTensor(x), [idx_train, idx_val, idx_test]))\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### Graph Convolution\n",
    "\n",
    "Here you are required the basic convolution operator of the GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQu9Yxfy-Wqj"
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, init='xavier'):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        if init == 'uniform':\n",
    "            print(\"| Uniform Initialization\")\n",
    "            self.init_uniform()\n",
    "        elif init == 'xavier':\n",
    "            print(\"| Xavier Initialization\")\n",
    "            self.init_xavier()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "\n",
    "    def init_uniform(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def init_xavier(self):\n",
    "        nn.init.xavier_normal_(self.weight.data, gain=0.02) # Implement Xavier Uniform\n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        \n",
    "        ########################################################################\n",
    "        ## START OF YOUR CODE\n",
    "        ########################################################################\n",
    "        \n",
    "     \n",
    "        ########################################################################\n",
    "        ## END OF YOUR CODE\n",
    "        ########################################################################\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### GCN Model\n",
    "\n",
    "Complete the GCN Model with one hidden layer with dropout, and the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, init):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        ########################################################################\n",
    "        ## START OF YOUR CODE\n",
    "        ########################################################################\n",
    "\n",
    "        \n",
    "        ########################################################################\n",
    "        ## END OF YOUR CODE\n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        ########################################################################\n",
    "        ## START OF YOUR CODE\n",
    "        ########################################################################\n",
    "        \n",
    "        \n",
    "        ########################################################################\n",
    "        ## END OF YOUR CODE\n",
    "        ########################################################################\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### Training\n",
    "\n",
    "\n",
    "First, we define the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr, lr_decay_epoch):\n",
    "    return lr * (0.5 ** (epoch / lr_decay_epoch))\n",
    "\n",
    "# Train\n",
    "def train(model, model_type, adj, features, labels, idx_train, idx_val, optimizer, epoch, lr, lr_decay_epoch, save_point):\n",
    "    global best_acc\n",
    "\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.lr = lr_scheduler(epoch, lr, lr_decay_epoch)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ########################################################################\n",
    "    ## START OF YOUR CODE\n",
    "    ########################################################################\n",
    "\n",
    "    \n",
    "    ########################################################################\n",
    "    ## END OF YOUR CODE\n",
    "    ########################################################################    \n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation for each epoch\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "    if acc_val > best_acc:\n",
    "        best_acc = acc_val\n",
    "        state = {\n",
    "            'model': model,\n",
    "            'acc': best_acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        \n",
    "        torch.save(state, os.path.join(save_point, '%s.t7' %(model_type)))\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\"=> Training Epoch #{} : lr = {:.4f}\".format(epoch, optimizer.lr))\n",
    "    sys.stdout.write(\" | Training acc : {:6.2f}%\".format(acc_train.data.cpu().numpy() * 100))\n",
    "    sys.stdout.write(\" | Best acc : {:.2f}%\". format(best_acc.data.cpu().numpy() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cora'\n",
    "num_hidden = 8 # number of features\n",
    "dropout = 0.6 # dropout\n",
    "weight_decay = 5e-4 # weight decay\n",
    "init_type = 'xavier' # [uniform | xavier]\n",
    "        \n",
    "lr = 5e-3 # initial learning rate\n",
    "optimizer_type = 'adam' # [sgd | adam]\n",
    "epoch = 800 # number of training epochs\n",
    "lr_decay_epoch = 5000 # multiply by a gamma every set iter\n",
    "alpha = 0.2 # Alpha value for the leaky_relu'\n",
    "\n",
    "\n",
    "# Data upload\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=dataroot, dataset=dataset)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "optimizer = None\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "print(\"| Constructing basic GCN model...\")\n",
    "model = GCN(\n",
    "        nfeat = features.shape[1],\n",
    "        nhid = num_hidden,\n",
    "        nclass = labels.max().item() + 1,\n",
    "        dropout = dropout,\n",
    "        init = init_type\n",
    ")\n",
    "\n",
    "\n",
    "if (optimizer_type == 'sgd'):\n",
    "    optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr = lr,\n",
    "            weight_decay = weight_decay,\n",
    "            momentum = 0.9\n",
    "    )\n",
    "elif (optimizer_type == 'adam'):\n",
    "    optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr = lr,\n",
    "            weight_decay = weight_decay\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "    features, adj, labels, idx_train, idx_val, idx_test = \\\n",
    "        list(map(lambda x: x.cuda(), [features, adj, labels, idx_train, idx_val, idx_test]))\n",
    "\n",
    "features, adj, labels = list(map(lambda x : Variable(x), [features, adj, labels]))\n",
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "\n",
    "save_point = os.path.join('./checkpoint', dataset)\n",
    "\n",
    "if not os.path.isdir(save_point):\n",
    "    os.mkdir(save_point)\n",
    "\n",
    "\n",
    "# Main code for training\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n[STEP 2] : Obtain (adjacency, feature, label) matrix\")\n",
    "    print(\"| Adjacency matrix : {}\".format(adj.shape))\n",
    "    print(\"| Feature matrix   : {}\".format(features.shape))\n",
    "    print(\"| Label matrix     : {}\".format(labels.shape))\n",
    "\n",
    "    # Training\n",
    "    print(\"\\n[STEP 3] : Training\")\n",
    "    for epoch in range(1, epoch+1):\n",
    "        train(model, 'gcn', adj, features, labels, idx_train, idx_val, optimizer, epoch, lr, lr_decay_epoch, save_point)\n",
    "    print(\"\\n=> Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### Test\n",
    "\n",
    "\n",
    "First, we define the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, adj, features, labels, idx_test):\n",
    "    print(\"\\n[STEP 4] : Testing\")\n",
    "    \n",
    "    ########################################################################\n",
    "    ## START OF YOUR CODE\n",
    "    ########################################################################\n",
    "\n",
    "    \n",
    "    ########################################################################\n",
    "    ## END OF YOUR CODE\n",
    "    ########################################################################\n",
    "    \n",
    "    print(output[idx_test].shape)\n",
    "    print(labels[idx_test].shape)\n",
    "\n",
    "    print(\"| Validation acc : {}%\".format(acc_val.data.cpu().numpy() * 100))\n",
    "    print(\"| Test acc : {}%\\n\".format(acc_test.data.cpu().numpy() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the performance of the model on unseen graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=dataroot, dataset=dataset)\n",
    "\n",
    "print(\"\\n[STEP 2] : Obtain (adjacency, feature, label) matrix\")\n",
    "print(\"| Adjacency matrix : {}\".format(adj.shape))\n",
    "print(\"| Feature matrix   : {}\".format(features.shape))\n",
    "print(\"| Label matrix     : {}\".format(labels.shape))\n",
    "\n",
    "load_model = torch.load(os.path.join('./checkpoint', dataset, '%s.t7' %('gcn')))\n",
    "model = load_model['model'].cpu()\n",
    "acc_val = load_model['acc']\n",
    "\n",
    "if use_gpu:\n",
    "    _, features, adj, labels, idx_test = \\\n",
    "            list(map(lambda x: x.cuda(), [model, features, adj, labels, idx_test]))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test(model, adj, features, labels, idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "## Part 1.2: GATs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### Graph Attention Networks\n",
    "\n",
    "We suggest the students to study and understand the equations for the attention mechanism in GNNs introduced in [Veličković et al.](https://arxiv.org/abs/1710.10903) (ICLR 2018), and also described in https://petar-v.com/GAT/ .\n",
    "\n",
    "<p align=\"center\"><img width=\"80%\" src=\"gat.jpg\"></p>\n",
    "\n",
    "Consider a graph of $N$ nodes, specified as a set of node features, $\\left( \\vec{h}_1, \\vec{h}_2, \\dots, \\vec{h}_N \\right), \\vec{h}_i \\in \\mathbb{R}^F$. A graph convolutional layer then computes a set of new node features, $\\left( \\vec{h}'_1, \\vec{h}'_2, \\dots, \\vec{h}'_N \\right), \\vec{h}'_i \\in \\mathbb{R}^{F'}$, based on the input features as well as the graph structure. Every graph convolutional layer starts off with a shared node-wise feature transformation (in order to achieve a higher-level representation), specified by a weight matrix $\\bf W\\in \\mathbb{R}^{\\textit{F}' \\times \\textit{F}}$. This transforms the feature vectors into $\\vec{g}_i = {\\bf W}\\vec{h}_i$. After this, the vectors $\\vec{g}_i $ are typically recombined in some way at each node.\n",
    "\n",
    "In general, to satisfy the localisation property, we can define a graph convolutional operator as an aggregation of features across neighbourhoods; defining $\\mathcal{N}_i$ as the neighbourhood of node $i$ (typically consisting of all first-order neighbours of $i$, including $i$ itself), we can define the output features of node $i$ as:\n",
    "\n",
    "$$\n",
    "\\vec{h}'_i = \\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\vec{g}_j\\right),\n",
    "$$\n",
    "\n",
    "where $\\alpha_{ij}$ specifies the weighting factor (importance) of node $j$'s features to node $i$. Generally, we let $\\alpha_{ij}$ be computed as a byproduct of an attentional mechanism, $a : \\mathbb{R}^N \\times \\mathbb{R}^N \\rightarrow \\mathbb{R}$, which computes unnormalised coefficients $e_{ij}$ across pairs of nodes $i,j$, based on their features:\n",
    "\n",
    "$$\n",
    "e_{ij} = a\\left( \\vec{h}_i, \\vec{h}_j \\right).\n",
    "$$\n",
    "\n",
    "The graph structure is injected by only allowing node $i$ to attend over nodes in its first-order neighbourhood, $j\\in \\mathcal{N}_i$. These coefficients are then typically normalised using the softmax function, in order to be comparable across different neighbourhoods:\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k\\in\\mathcal{N}_i}\\exp(e_{ik})}.\n",
    "$$\n",
    "\n",
    "In  [Veličković et al.](https://arxiv.org/abs/1710.10903), the  attention  mechanism $a$ is a  single-layer  feedforward  neural  network, parametrised by a weight vector $\\vec{{\\bf a}}\\in \\mathbb{R}^{2F'}$, and applying the LeakyReLU nonlinearity (with negative input slope $\\alpha= 0.2$).  Fully expanded out, the coefficients computed by the attention mechanism are then be expressed as:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\vec{\\bf a}^T\\left[{\\bf W} \\vec{h}_i || {\\bf W} \\vec{h}_j\\right]\\right)\\right)}{\\sum_{k\\in\\mathcal{N}_i}\\exp\\left(\\text{LeakyReLU}\\left(\\vec{\\bf a}^T\\left[ {\\bf W} \\vec{h}_i || {\\bf W} \\vec{h}_k\\right]\\right)\\right)},\n",
    "$$\n",
    "\n",
    "where $||$ is the concatenation operation.\n",
    "\n",
    "To stabilise the learning process of self-attention, multi-head attention turns out to be very beneficial. Namely, the operations of the layer are independently replicated $K$ times (each replica with different parameters), and outputs are feature-wise aggregated (typically by concatenating or adding):\n",
    "\n",
    "$$\n",
    "\\vec{h}'_i = {\\LARGE \\|}_{k=1}^K \\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}^k{\\bf W}^k\\vec{h}_j\\right),\n",
    "$$\n",
    "\n",
    "where $α^k_{ij}$ are the attention coefficients derived by the $k$-th replica, and ${\\bf W}^k$ the weight matrix specifying the linear transformation of the $k$-th replica.\n",
    "\n",
    "Now you have to define the basic operators of the GAT layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQu9Yxfy-Wqj"
   },
   "outputs": [],
   "source": [
    "class GraphAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, init='xavier', concat=True):\n",
    "        super(GraphAttention, self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "                \n",
    "        self.W = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.a1 = nn.Parameter(torch.Tensor(out_features, 1))\n",
    "        self.a2 = nn.Parameter(torch.Tensor(out_features, 1))\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        \n",
    "        if init == 'uniform':\n",
    "            print(\"| Uniform Initialization\")\n",
    "            self.init_uniform()\n",
    "        elif init == 'xavier':\n",
    "            print(\"| Xavier Initialization\")\n",
    "            self.init_xavier()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "\n",
    "    def init_uniform(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(1))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        self.a1.data.uniform_(-stdv, stdv)\n",
    "        self.a2.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def init_xavier(self):\n",
    "        nn.init.xavier_normal_(self.W.data, gain=0.02) # Implement Xavier Uniform\n",
    "        nn.init.xavier_normal_(self.a1.data, gain=0.02) # Implement Xavier Uniform\n",
    "        nn.init.xavier_normal_(self.a2.data, gain=0.02) # Implement Xavier Uniform\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        \n",
    "        ########################################################################\n",
    "        ## START OF YOUR CODE\n",
    "        ########################################################################\n",
    "        \n",
    "        \n",
    "        ########################################################################\n",
    "        ## END OF YOUR CODE\n",
    "        ########################################################################\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### GAT Model\n",
    "\n",
    "You have to build the GAT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, init, alpha, nheads):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        ########################################################################\n",
    "        ## START OF YOUR CODE\n",
    "        ########################################################################\n",
    "        \n",
    "    \n",
    "        ########################################################################\n",
    "        ## END OF YOUR CODE\n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        ########################################################################\n",
    "        ## START OF YOUR CODE\n",
    "        ########################################################################\n",
    "        \n",
    "        \n",
    "        ########################################################################\n",
    "        ## END OF YOUR CODE\n",
    "        ########################################################################\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### Training\n",
    "Next we run the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cora' \n",
    "num_hidden = 8 # number of features\n",
    "dropout = 0.6 # dropout\n",
    "weight_decay = 5e-4 # weight decay\n",
    "init_type = 'xavier' # [uniform | xavier]\n",
    "        \n",
    "lr = 5e-3 # initial learning rate\n",
    "optimizer_type = 'adam' # [sgd | adam]\n",
    "epoch = 800 # number of training epochs\n",
    "lr_decay_epoch = 5000 # multiply by a gamma every set iter\n",
    "nb_heads = 8 # number of head attentions\n",
    "alpha = 0.2 # Alpha value for the leaky_relu'\n",
    "\n",
    "\n",
    "# Data upload\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=dataroot, dataset=dataset)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "optimizer = None\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "# Define the model and optimizer\n",
    "print(\"| Constructing Attention model...\")\n",
    "model = GAT(\n",
    "        nfeat = features.shape[1],\n",
    "        nhid = num_hidden,\n",
    "        nclass = int(labels.max().item()) + 1,\n",
    "        dropout = dropout,\n",
    "        nheads = nb_heads,\n",
    "        alpha = alpha,\n",
    "        init = init_type\n",
    ")\n",
    "\n",
    "\n",
    "if (optimizer_type == 'sgd'):\n",
    "    optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr = lr,\n",
    "            weight_decay = weight_decay,\n",
    "            momentum = 0.9\n",
    "    )\n",
    "elif (optimizer_type == 'adam'):\n",
    "    optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr = lr,\n",
    "            weight_decay = weight_decay\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "    features, adj, labels, idx_train, idx_val, idx_test = \\\n",
    "        list(map(lambda x: x.cuda(), [features, adj, labels, idx_train, idx_val, idx_test]))\n",
    "\n",
    "features, adj, labels = list(map(lambda x : Variable(x), [features, adj, labels]))\n",
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "\n",
    "save_point = os.path.join('./checkpoint', dataset)\n",
    "\n",
    "if not os.path.isdir(save_point):\n",
    "    os.mkdir(save_point)\n",
    "\n",
    "\n",
    "\n",
    "# Main code for training\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n[STEP 2] : Obtain (adjacency, feature, label) matrix\")\n",
    "    print(\"| Adjacency matrix : {}\".format(adj.shape))\n",
    "    print(\"| Feature matrix   : {}\".format(features.shape))\n",
    "    print(\"| Label matrix     : {}\".format(labels.shape))\n",
    "\n",
    "    # Training\n",
    "    print(\"\\n[STEP 3] : Training\")\n",
    "    for epoch in range(1, epoch+1):\n",
    "        train(model, 'gat', adj, features, labels, idx_train, idx_val, optimizer, epoch, lr, lr_decay_epoch, save_point)\n",
    "    print(\"\\n=> Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### Test\n",
    "\n",
    "\n",
    "We now evaluate the performance of the GAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=dataroot, dataset=dataset)\n",
    "\n",
    "print(\"\\n[STEP 2] : Obtain (adjacency, feature, label) matrix\")\n",
    "print(\"| Adjacency matrix : {}\".format(adj.shape))\n",
    "print(\"| Feature matrix   : {}\".format(features.shape))\n",
    "print(\"| Label matrix     : {}\".format(labels.shape))\n",
    "\n",
    "load_model = torch.load(os.path.join('./checkpoint', dataset, '%s.t7' %('gat')))\n",
    "model = load_model['model'].cpu()\n",
    "acc_val = load_model['acc']\n",
    "\n",
    "if use_gpu:\n",
    "    _, features, adj, labels, idx_test = \\\n",
    "            list(map(lambda x: x.cuda(), [model, features, adj, labels, idx_test]))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test(model, adj, features, labels, idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0H6oJyOX-W7n"
   },
   "source": [
    "# Part 2: Theoretical questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XdGBKQU0kJYx"
   },
   "source": [
    "Please answer the following theoretical questions in a cell below each respective question.\n",
    "\n",
    "a. What is the motivation for utilising self-attention in GATs instead of the GCN convolutional operator?\n",
    "- Self-attention is state-of-the-art in text analysis (cf. Transformer model), plus it allows us to learn the edge weights instead of utilising (being limited by) the data adjacency matrix.\n",
    "\n",
    "b. Explain multi-head attention in the context of GATs and provide a reason for why we elect to use it instead of the single-head baseline variant.\n",
    "\n",
    "- We simply define multiple weight matrices for the calculation of the unnormalised attention/similarity scores and perform the self-attention multiple times (once per matrix defined). Multi-head attention helps stabilise the training process and is, thus, preferred."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN tutorial and coursework.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
